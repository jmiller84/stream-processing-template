{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/13 13:01:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, avg, sum, window\n",
    "import time\n",
    "\n",
    "# Define the path to the jars on the EC2 instance\n",
    "spark_jars_path = \"/home/ec2-user/stream-processing-template/jars\"  # <-- Update this path\n",
    "\n",
    "spark = SparkSession.builder.appName(\"retail_pysaprk_consumer\") \\\n",
    "    .config(\"spark.jars\", f\"{spark_jars_path}/commons-pool2-2.11.1.jar,\"\n",
    "            f\"{spark_jars_path}/spark-sql-kafka-0-10_2.12-3.4.0.jar,\"\n",
    "            f\"{spark_jars_path}/spark-streaming-kafka-0-10-assembly_2.12-3.4.0.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for our data (Column names and Datatype for each column)\n",
    "schema = StructType([\n",
    "    StructField(\"store_location\", StringType(), True),\n",
    "    StructField(\"time_of_purchase\", TimestampType(), True),\n",
    "    StructField(\"product_ID\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Stream from Kafka topic\n",
    "\n",
    "# initalise a spark object and read the stream of data using the readStream method\n",
    "# set the format of the streaming source to kafka\n",
    "# set the kafka topic to retail_transactions (topic is a feed name to which messages are published)\n",
    "# load the data and store it in the df variable\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"b-1.monstercluster1.6xql65.c3.kafka.eu-west-2.amazonaws.com:9092\") \\\n",
    "    .option(\"subscribe\", \"retail_transactions\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/13 13:01:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-89651826-c9c9-4627-9a2d-1898c24a8485. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/12/13 13:01:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/12/13 13:01:39 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract and parse the JSON data - convert the json data to strings and add it to a new column 'data'\n",
    "\n",
    "# selectExpr allows you to select and transform columns using SQL\n",
    "# CAST(value AS STRING) - SQL expression that set the datatype for the selected column as a string\n",
    "# withColumn - Pyspark method used to add a or replace a column to the DF, in this case creating a new column named 'data'\n",
    "#Â the from_json function takes the json data from the value column of the stream and inserts into the new column\n",
    "transactions = (df.selectExpr(\"CAST(value AS STRING)\")\n",
    "                .withColumn(\"data\", from_json(col(\"value\"), schema))\n",
    "                .select(\"data.*\"))\n",
    "\n",
    "\n",
    "# write the transactions DF to an in memory table called temporary_view_two\n",
    "\n",
    "# writeStream - write the streaming data to memory, whereas readStream reads streaming data\n",
    "# set the format to memory\n",
    "# give the query a name using queryName\n",
    "# start the streaming query\n",
    "query = transactions.writeStream \\\n",
    ".format(\"memory\") \\\n",
    ".queryName(\"temporary_view_two\") \\\n",
    ".start()\n",
    "\n",
    "query.awaitTermination(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = spark.sql(\"SELECT * FROM temporary_view_two\")\n",
    "\n",
    "# Now you can perform aggregations or other transformations on `processed_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+------------------+\n",
      "|store_location|   time_of_purchase|product_ID|transaction_amount|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "|       Phoenix|2023-12-13 13:01:42|    P00080|               513|\n",
      "|   San Antonio|2023-12-13 13:01:44|    P00077|               470|\n",
      "|       Chicago|2023-12-13 13:01:47|    P00026|               436|\n",
      "|       Chicago|2023-12-13 13:01:49|    P00062|                28|\n",
      "|  Philadelphia|2023-12-13 13:01:52|    P00093|               537|\n",
      "|   San Antonio|2023-12-13 13:01:54|    P00097|                 9|\n",
      "|      New York|2023-12-13 13:01:56|    P00051|               415|\n",
      "|       Phoenix|2023-12-13 13:01:58|    P00054|               998|\n",
      "|       Chicago|2023-12-13 13:02:00|    P00016|                51|\n",
      "|       Houston|2023-12-13 13:02:03|    P00065|               789|\n",
      "|   Los Angeles|2023-12-13 13:02:06|    P00064|               140|\n",
      "|       Phoenix|2023-12-13 13:02:08|    P00028|               467|\n",
      "|       Chicago|2023-12-13 13:02:10|    P00023|               150|\n",
      "|       Phoenix|2023-12-13 13:02:11|    P00004|                57|\n",
      "|  Philadelphia|2023-12-13 13:02:14|    P00076|               421|\n",
      "|  Philadelphia|2023-12-13 13:02:16|    P00004|               145|\n",
      "|       Chicago|2023-12-13 13:02:17|    P00042|               797|\n",
      "|      New York|2023-12-13 13:02:19|    P00053|               360|\n",
      "|   San Antonio|2023-12-13 13:02:20|    P00083|               341|\n",
      "|   Los Angeles|2023-12-13 13:02:23|    P00068|               493|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "transactions = processed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of transactions per minute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2023-12-13 13:01...|    8|\n",
      "|{2023-12-13 13:02...|   30|\n",
      "|{2023-12-13 13:03...|   29|\n",
      "|{2023-12-13 13:04...|   30|\n",
      "|{2023-12-13 13:05...|   30|\n",
      "|{2023-12-13 13:06...|   30|\n",
      "|{2023-12-13 13:07...|   33|\n",
      "|{2023-12-13 13:08...|   31|\n",
      "|{2023-12-13 13:09...|   30|\n",
      "|{2023-12-13 13:10...|   27|\n",
      "|{2023-12-13 13:11...|   30|\n",
      "|{2023-12-13 13:12...|   28|\n",
      "|{2023-12-13 13:13...|   30|\n",
      "|{2023-12-13 13:14...|   31|\n",
      "|{2023-12-13 13:15...|   30|\n",
      "|{2023-12-13 13:16...|   30|\n",
      "|{2023-12-13 13:17...|   28|\n",
      "|{2023-12-13 13:18...|   29|\n",
      "|{2023-12-13 13:19...|   29|\n",
      "|{2023-12-13 13:20...|   30|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_trans_per_min = processed_data.groupBy(window(\"time_of_purchase\", \"1 minute\")).count()\n",
    "num_trans_per_min = num_trans_per_min.orderBy(col('window').asc())\n",
    "num_trans_per_min.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total of all transactions per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|              window|sum(transaction_amount)|\n",
      "+--------------------+-----------------------+\n",
      "|{2023-12-13 13:01...|                   3406|\n",
      "|{2023-12-13 13:02...|                  11386|\n",
      "|{2023-12-13 13:03...|                  15193|\n",
      "|{2023-12-13 13:04...|                  10881|\n",
      "|{2023-12-13 13:05...|                  11320|\n",
      "|{2023-12-13 13:06...|                  10128|\n",
      "|{2023-12-13 13:07...|                  14988|\n",
      "|{2023-12-13 13:08...|                  12621|\n",
      "|{2023-12-13 13:09...|                  15401|\n",
      "|{2023-12-13 13:10...|                  12549|\n",
      "|{2023-12-13 13:11...|                  12769|\n",
      "|{2023-12-13 13:12...|                  10432|\n",
      "|{2023-12-13 13:13...|                  15261|\n",
      "|{2023-12-13 13:14...|                  16939|\n",
      "|{2023-12-13 13:15...|                  15157|\n",
      "|{2023-12-13 13:16...|                  12890|\n",
      "|{2023-12-13 13:17...|                  14218|\n",
      "|{2023-12-13 13:18...|                  15278|\n",
      "|{2023-12-13 13:19...|                  17248|\n",
      "|{2023-12-13 13:20...|                   9990|\n",
      "+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_trans_amount_per_min = processed_data.groupBy(window(\"time_of_purchase\", \"1 minute\")).sum(\"transaction_amount\")\n",
    "total_trans_amount_per_min = total_trans_amount_per_min.orderBy(col('window').asc())\n",
    "total_trans_amount_per_min.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top N Products by Number of tranactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_Id|count|\n",
      "+----------+-----+\n",
      "|    P00098|   18|\n",
      "|    P00019|   17|\n",
      "|    P00004|   14|\n",
      "|    P00037|   13|\n",
      "|    P00047|   13|\n",
      "|    P00001|   13|\n",
      "|    P00003|   12|\n",
      "|    P00078|   11|\n",
      "|    P00082|   11|\n",
      "|    P00086|   11|\n",
      "|    P00063|   11|\n",
      "|    P00094|   11|\n",
      "|    P00065|   10|\n",
      "|    P00075|   10|\n",
      "|    P00023|   10|\n",
      "|    P00070|   10|\n",
      "|    P00091|   10|\n",
      "|    P00089|   10|\n",
      "|    P00069|   10|\n",
      "|    P00054|    9|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_products = processed_data.groupBy(col('product_Id')).count()\n",
    "top_products = top_products.orderBy(col('count').desc())\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top products by sales amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|product_Id|sum(transaction_amount)|\n",
      "+----------+-----------------------+\n",
      "|    P00019|                   9153|\n",
      "|    P00086|                   7858|\n",
      "|    P00098|                   7836|\n",
      "|    P00047|                   7239|\n",
      "|    P00050|                   6410|\n",
      "|    P00096|                   6248|\n",
      "|    P00018|                   5851|\n",
      "|    P00075|                   5799|\n",
      "|    P00063|                   5774|\n",
      "|    P00070|                   5581|\n",
      "|    P00035|                   5490|\n",
      "|    P00065|                   5485|\n",
      "|    P00001|                   5406|\n",
      "|    P00064|                   5384|\n",
      "|    P00023|                   5361|\n",
      "|    P00097|                   5309|\n",
      "|    P00082|                   5191|\n",
      "|    P00081|                   5164|\n",
      "|    P00069|                   4972|\n",
      "|    P00071|                   4926|\n",
      "+----------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_prod_by_sales_amount = processed_data.groupBy(col('product_Id')).sum('transaction_amount')\n",
    "top_prod_by_sales_amount = top_prod_by_sales_amount.orderBy(col('sum(transaction_amount)').desc())\n",
    "top_prod_by_sales_amount.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomoly Detection \n",
    "\n",
    "- Identify transactions with amounts significantly different from the average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average transaction ammount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453.2670401493931"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_trans = processed_data.agg(avg(col('transaction_amount')).alias('avg_transaction_amount')).collect()[0]['avg_transaction_amount']\n",
    "avg_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for transactions with a transaction amount 50% larger than the average transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+------------------+\n",
      "|store_location|   time_of_purchase|product_ID|transaction_amount|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "|       Phoenix|2023-12-13 13:01:58|    P00054|               998|\n",
      "|       Houston|2023-12-13 13:02:03|    P00065|               789|\n",
      "|       Chicago|2023-12-13 13:02:17|    P00042|               797|\n",
      "|   San Antonio|2023-12-13 13:02:25|    P00035|               765|\n",
      "|   Los Angeles|2023-12-13 13:02:27|    P00050|               719|\n",
      "|  Philadelphia|2023-12-13 13:02:31|    P00024|               722|\n",
      "|   San Antonio|2023-12-13 13:02:33|    P00096|               910|\n",
      "|       Phoenix|2023-12-13 13:02:37|    P00075|               941|\n",
      "|       Houston|2023-12-13 13:02:50|    P00060|               730|\n",
      "|   Los Angeles|2023-12-13 13:03:01|    P00086|               934|\n",
      "|       Chicago|2023-12-13 13:03:08|    P00100|               861|\n",
      "|      New York|2023-12-13 13:03:14|    P00033|               725|\n",
      "|       Chicago|2023-12-13 13:03:19|    P00019|               740|\n",
      "|  Philadelphia|2023-12-13 13:03:21|    P00051|               841|\n",
      "|      New York|2023-12-13 13:03:31|    P00015|               874|\n",
      "|  Philadelphia|2023-12-13 13:03:46|    P00036|               946|\n",
      "|  Philadelphia|2023-12-13 13:03:48|    P00038|               896|\n",
      "|   San Antonio|2023-12-13 13:03:53|    P00066|               854|\n",
      "|       Chicago|2023-12-13 13:03:56|    P00085|               851|\n",
      "|      New York|2023-12-13 13:04:03|    P00081|               683|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anomoly_detect = processed_data.filter(col('transaction_amount') > avg_trans * 1.5)\n",
    "anomoly_detect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter all transactions above and below a certain amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------+------------------+\n",
      "|store_location|   time_of_purchase|product_ID|transaction_amount|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "|       Houston|2023-12-13 13:02:03|    P00065|               789|\n",
      "|       Chicago|2023-12-13 13:02:17|    P00042|               797|\n",
      "|   San Antonio|2023-12-13 13:02:25|    P00035|               765|\n",
      "|   San Antonio|2023-12-13 13:04:11|    P00050|               786|\n",
      "|   Los Angeles|2023-12-13 13:05:22|    P00080|               755|\n",
      "|   San Antonio|2023-12-13 13:06:14|    P00070|               786|\n",
      "|       Phoenix|2023-12-13 13:06:25|    P00019|               773|\n",
      "|  Philadelphia|2023-12-13 13:07:00|    P00086|               783|\n",
      "|       Phoenix|2023-12-13 13:07:02|    P00062|               751|\n",
      "|  Philadelphia|2023-12-13 13:07:31|    P00027|               796|\n",
      "|       Chicago|2023-12-13 13:07:45|    P00067|               796|\n",
      "|       Chicago|2023-12-13 13:08:56|    P00018|               780|\n",
      "|   Los Angeles|2023-12-13 13:08:57|    P00048|               754|\n",
      "|  Philadelphia|2023-12-13 13:09:11|    P00009|               798|\n",
      "|       Phoenix|2023-12-13 13:09:13|    P00069|               794|\n",
      "|   Los Angeles|2023-12-13 13:09:20|    P00014|               779|\n",
      "|      New York|2023-12-13 13:09:56|    P00072|               787|\n",
      "|  Philadelphia|2023-12-13 13:10:23|    P00040|               756|\n",
      "|   San Antonio|2023-12-13 13:10:39|    P00004|               751|\n",
      "|       Houston|2023-12-13 13:11:02|    P00084|               784|\n",
      "+--------------+-------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitlered_trans = processed_data.filter((col('transaction_amount') < 800) & (col('transaction_amount') > 750))\n",
    "fitlered_trans.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
